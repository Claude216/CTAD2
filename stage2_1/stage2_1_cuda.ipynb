{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the cuda version of the stage2_1.ipynb\n",
    "# we will use the cuda to accelerate the calculation of the temperature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chip_temp_pred_models as ctpm\n",
    "import torch\n",
    "import pandas\n",
    "import utilities\n",
    "import numpy as np\n",
    "import math\n",
    "import temperature_calculator as temp_cal\n",
    "import interconnect_calculator as inter_cal\n",
    "import random\n",
    "import copy\n",
    "from multiprocessing import Pool\n",
    "from concurrent.futures import ProcessPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the beginning of the file\n",
    "DEVICE = 'cpu' # torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlap constraints\n",
    "\n",
    "def is_2_chip_overlapping_cuda(chip1, chip2, min_edge_dist:float=0.2): \n",
    "    \"\"\"\n",
    "    Check if two chiplets are overlapping.\n",
    "    input: \n",
    "        chip1: the first chiplet\n",
    "        chip2: the second chiplet\n",
    "        min_edge_dist: the minimum edge distance between two chiplets\n",
    "    output: \n",
    "        True: if two chiplets are overlapping\n",
    "        False: if two chiplets are not overlapping\n",
    "    \"\"\"\n",
    "    # x1, y1 = chip1[0], chip1[1]\n",
    "    # w1, h1 = chip1[2], chip1[3]\n",
    "    # x2, y2 = chip2[0], chip2[1]\n",
    "    # w2, h2 = chip2[2], chip2[3]\n",
    "\n",
    "    x1, y1, w1, h1 = \\\n",
    "        chip1[0].item(), chip1[1].item(), chip1[2].item(), chip1[3].item()\n",
    "    try: \n",
    "        x2, y2, w2, h2 = \\\n",
    "            chip2[0].item(), chip2[1].item(), chip2[2].item(), chip2[3].item()\n",
    "    except: \n",
    "        print(chip2)\n",
    "\n",
    "    no_overlap = \\\n",
    "        (x1 + w1 + min_edge_dist <= x2 or \\\n",
    "            x2 + w2 + min_edge_dist <= x1) or \\\n",
    "        (y1 + h1 + min_edge_dist <= y2 or \\\n",
    "            y2 + h2 + min_edge_dist <= y1)\n",
    "      \n",
    "    \n",
    "    return not no_overlap\n",
    "\n",
    "def check_overlap_within_layout_cuda(layout):\n",
    "    \"\"\"\n",
    "    Check if there are overlapping chiplets within the layout.\n",
    "    input: \n",
    "        layout: the layout of the chiplets\n",
    "        chiplets: the chiplets\n",
    "    output: \n",
    "        True: if there are overlapping chiplets\n",
    "        False: if there are no overlapping chiplets\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to tensor if not already\n",
    "    if not torch.is_tensor(layout):\n",
    "        layout = torch.tensor(layout, device=DEVICE)\n",
    "    \n",
    "    n = len(layout)\n",
    "    # Create tensors for all pairs of chiplets\n",
    "    i_indices, j_indices = torch.triu_indices(n, n, offset=1)\n",
    "    \n",
    "    # Vectorized overlap checking\n",
    "    overlaps = torch.stack([\n",
    "        torch.tensor(is_2_chip_overlapping_cuda(layout[i], layout[j]))\n",
    "        for i, j in zip(i_indices, j_indices)\n",
    "    ])\n",
    "    \n",
    "    return overlaps.any()\n",
    "\n",
    "def is_1_chip_overlapping_with_layout(curr_chip, layout): \n",
    "    overlaps = torch.stack([\n",
    "        is_2_chip_overlapping_cuda(curr_chip, chip)\n",
    "        for chip in layout\n",
    "    ])\n",
    "\n",
    "    return overlaps.any()\n",
    "\n",
    "def is_overlapping_in_init_grid_cuda(x_c, y_c, len_c, wid_c, index_c, layout, min_edge_dist): \n",
    "    \"\"\"\n",
    "    Check if a chiplet is overlapping with other chiplets in the layout.\n",
    "    \"\"\"\n",
    "\n",
    "    curr_chip = torch.tensor([x_c, y_c, len_c, wid_c, index_c], device=DEVICE)\n",
    "    n = len(layout)\n",
    "    i_indices = range(n)\n",
    "    \n",
    "    overlaps = torch.stack([\n",
    "        torch.tensor(is_2_chip_overlapping_cuda(curr_chip, layout[i]), device=DEVICE)\n",
    "        for i in i_indices\n",
    "    ])\n",
    "\n",
    "    return overlaps.any()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general helper function\n",
    "def get_shape_of_board_cuda(layout, chiplets, margin_width=1.0): \n",
    "    \"\"\"\n",
    "    Get the shape of the board.\n",
    "    input: \n",
    "        layout: the layout of the chiplets\n",
    "        chiplets: the chiplets\n",
    "        margin_width: the margin width of the board\n",
    "    output: \n",
    "        the shape of the board: length and width of the board\n",
    "    \"\"\"\n",
    "    x_max, x_min, y_max, y_min = float('-inf'), float('inf'),float('-inf'),float('inf')\n",
    "\n",
    "    for chip in layout: \n",
    "        # chiplet_index = int(chip[-1].item())\n",
    "        x_max = max(x_max, chip[0].item() + chip[2].item())\n",
    "        x_min = min(x_min, chip[0].item())\n",
    "        y_max = max(y_max, chip[1].item() + chip[3].item())\n",
    "        y_min = min(y_min, chip[1].item())\n",
    "\n",
    "    board_len = abs(x_max - x_min) + 2 * margin_width\n",
    "    board_wid = abs(y_max - y_min) + 2 * margin_width\n",
    "\n",
    "    return [board_len, board_wid]\n",
    "\n",
    "def get_chiplet_dict_cuda(chip_length, chip_width, Convection_Film_Coefficient, Internal_Heat_Generation_Magnitude): \n",
    "    \"\"\"\n",
    "    Create a dictionary of a chiplet.\n",
    "    \"\"\"\n",
    "    chip = dict()\n",
    "    chip['len'] = chip_length\n",
    "    chip['wid'] = chip_width\n",
    "    chip['CFC'] = Convection_Film_Coefficient\n",
    "    chip['IHGM'] = Internal_Heat_Generation_Magnitude\n",
    "\n",
    "    chip['A'], chip['k'] = ctpm.get_out_chip_decay_curve_coef(chip_len=chip['len'],\n",
    "                                                    chip_wid=chip['wid'],\n",
    "                                                    Convection_Film_Coefficient=chip['CFC'],\n",
    "                                                    Internal_Heat_Generation_Magnitude=chip['IHGM'])\n",
    "\n",
    "    return chip\n",
    "\n",
    "def generate_a_layout_cuda(grid_length, grid_width, chiplets: dict, min_edge_distance: float=2.0, margin_width: float=1.0): \n",
    "    \"\"\"\n",
    "    Generate a layout of chiplets.\n",
    "    input: \n",
    "        grid_length: the length of the grid\n",
    "        grid_width: the width of the grid\n",
    "        chiplets: the chiplets\n",
    "        min_edge_distance: the minimum edge distance between two chiplets\n",
    "        margin_width: the margin width of the board\n",
    "    output: \n",
    "        the layout of the chiplets\n",
    "    \"\"\"\n",
    "    layout = torch.tensor([])\n",
    "\n",
    "    for index, chip in chiplets.items(): \n",
    "        placed = False\n",
    "        \n",
    "        while not placed: \n",
    "            if len(layout) >= 1:  \n",
    "                while True: \n",
    "                    x = random.randint(0, grid_length - chip['len'])\n",
    "                    y = random.randint(0, grid_width - chip['wid'])\n",
    "                    if not is_overlapping_in_init_grid_cuda(x, y, chip['len'], chip['wid'], index, layout, min_edge_dist=min_edge_distance): \n",
    "                        # layout.append([x, y, chip['len'], chip['wid'], chip['len'] >= chip['wid'], index])\n",
    "                        chip_tensor = torch.tensor([x, y, chip['len'], chip['wid'], index])\n",
    "                        layout = torch.cat((layout, chip_tensor.unsqueeze(0)), dim=0)\n",
    "                        placed = True\n",
    "                        break\n",
    "            else: \n",
    "                x = random.randint(0, grid_length - chip['len'])\n",
    "                y = random.randint(0, grid_width - chip['wid'])\n",
    "                # layout.append([x, y, chip['len'], chip['wid'], index])\n",
    "                chip_tensor = torch.tensor([x, y, chip['len'], chip['wid'], index])\n",
    "                layout = torch.cat((layout, chip_tensor.unsqueeze(0)), dim=0)\n",
    "                placed = True\n",
    "\n",
    "    # 裁剪芯片板\n",
    "    # for chip in layout: \n",
    "    # x_max = max(x_max, chip[0] + chiplets[chip[-1]]['len'])\n",
    "    x_min = min(chip[0] for chip in layout)\n",
    "    # y_max = max(y_max, chip[1] + chiplets[chip[-1]]['wid'])\n",
    "    y_min = min(chip[1] for chip in layout)\n",
    "\n",
    "    x_move, y_move = x_min - margin_width, y_min - margin_width\n",
    "\n",
    "    # 更新芯片定位点位置\n",
    "    # for i in range(len(layout)): \n",
    "    #     layout[i][0] -= x_move\n",
    "    #     layout[i][1] -= y_move\n",
    "    layout[:, 0] -= x_move\n",
    "    layout[:, 1] -= y_move\n",
    "    return layout\n",
    "\n",
    "def get_dynamic_coefficient(current_epoch, total_epochs, base=10.0):\n",
    "    # e.g. exponential decay:\n",
    "    return base * math.exp(-5.0 * current_epoch / total_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GA part\n",
    "def distance_center_to_chip(cx, cy, x_i, y_i, length, width):\n",
    "    x_min = x_i\n",
    "    x_max = x_i + length\n",
    "    y_min = y_i\n",
    "    y_max = y_i + width\n",
    "\n",
    "    # horizontal distance from c.x to chip bounding box\n",
    "    if cx < x_min:\n",
    "        dist_x = x_min - cx\n",
    "    elif cx > x_max:\n",
    "        dist_x = cx - x_max\n",
    "    else:\n",
    "        dist_x = 0.0\n",
    "\n",
    "    # vertical distance from c.y to chip bounding box\n",
    "    if cy < y_min:\n",
    "        dist_y = y_min - cy\n",
    "    elif cy > y_max:\n",
    "        dist_y = cy - y_max\n",
    "    else:\n",
    "        dist_y = 0.0\n",
    "\n",
    "    return math.sqrt(dist_x**2 + dist_y**2)\n",
    "\n",
    "def compute_center_distance_penalty(layout, cx, cy):\n",
    "    penalty_sum = 0.0\n",
    "    big_penalty_if_overlap = 10000.0\n",
    "\n",
    "    for chip in layout:\n",
    "        x_i, y_i = chip[0].item(), chip[1].item()\n",
    "        length, width = chip[2].item(), chip[3].item()\n",
    "\n",
    "        # Check if center is inside bounding box\n",
    "        if (x_i <= cx <= x_i + length) and (y_i <= cy <= y_i + width):\n",
    "            penalty_sum += big_penalty_if_overlap\n",
    "        else:\n",
    "            dist = distance_center_to_chip(cx, cy, x_i, y_i, length, width)\n",
    "            penalty_sum += dist  # or dist**2\n",
    "\n",
    "    return penalty_sum\n",
    "\n",
    "\n",
    "def fitness_cuda(layout, connectivity, chiplets):\n",
    "    # Convert layout to tensor and move to GPU\n",
    "    if not torch.is_tensor(layout): \n",
    "        layout = torch.tensor(layout, device=DEVICE)\n",
    "    # layout_tensor = torch.tensor(layout, DEVICE=device, dtype=torch.float32)\n",
    "    \n",
    "    # inter_start = time.time()\n",
    "    # Convert interconnect calculations to use tensors\n",
    "    inter_connect_length = inter_cal.get_total_interconnect_length_cuda(\n",
    "        layout=layout, \n",
    "        connectivity_pairs=connectivity\n",
    "    )\n",
    "    # print(f\"Inter connect score calculated, time consume: {time.time() - inter_start:.4f} seconds\")\n",
    "\n",
    "\n",
    "    temp_start = time.time()\n",
    "    # Temperature calculations on GPU\n",
    "    max_temp, temp_uniformity = temp_cal.get_max_temp_and_temp_uniformity_cuda(\n",
    "        layout, chiplets\n",
    "    )\n",
    "    # print(f\"Temp score calculated, time consumed: {time.time() - temp_start:.4f}\")\n",
    "    \n",
    "    # Calculate overlap penalty using tensor operations\n",
    "    overlap_penalty = 10000 if check_overlap_within_layout_cuda(layout) else 0\n",
    "    \n",
    "    # area_len, area_wid = get_shape_of_board_cuda(layout, chiplets)\n",
    "    \n",
    "    \n",
    "    # shape penalty\n",
    "    # shape_ratio = area_len / area_wid\n",
    "    # shape_penalty = 5.0 * (shape_ratio + 1.0/shape_ratio +  2.0)\n",
    "\n",
    "\n",
    "    # empty area penalty\n",
    "    # chip_area = sum([chip[2].item() * chip[3].item() \\\n",
    "    #                  for chip in layout])\n",
    "    # area_penalty = 0.1 * (area_len * area_wid - chip_area)\n",
    "\n",
    "\n",
    "    # center_dist_penalty\n",
    "    # cx, cy = area_len/2, area_wid/2\n",
    "    # center_dist_penalty = 2.5 * compute_center_distance_penalty(layout, cx, cy)\n",
    "\n",
    "\n",
    "    # spread penalty\n",
    "    coords = torch.stack([\n",
    "        layout[:, 0] + layout[:, 2]*0.5,  # center X\n",
    "        layout[:, 1] + layout[:, 3]*0.5   # center Y\n",
    "    ], dim=1)\n",
    "    centroid = coords.mean(dim=0)\n",
    "    diff = coords - centroid\n",
    "    dist_sq = diff[:, 0]**2 + diff[:, 1]**2\n",
    "    spread_penalty = dist_sq.sum()  # sum of squared distances\n",
    "    spread_weight = 0.2\n",
    "    spread_penalty *= spread_weight\n",
    "\n",
    "    # print(\"Inter connect length: \", inter_connect_length)\n",
    "    # print(\"overlap penalty\", overlap_penalty)\n",
    "    # # print(\"shape_penalty\", shape_penalty)\n",
    "    # # print(\"area penalty\", area_penalty)\n",
    "    # print(\"spread_penalty\", spread_penalty)\n",
    "    # print(\"temperature_uniformity\", temp_uniformity)\n",
    "    # print(\"center_dist_penalty\", center_dist_penalty)\n",
    "    return inter_connect_length + \\\n",
    "            temp_uniformity + \\\n",
    "            overlap_penalty + \\\n",
    "            spread_penalty \n",
    "\n",
    "def select_cuda(population, fitnesses):\n",
    "    \"\"\"\n",
    "    Select two individuals from the population based on their fitness.\n",
    "    \"\"\"\n",
    "    # return random.choices(population, weights=fitnesses, k=2)\n",
    "    # random_indices = torch.randperm(population.size(0))[:2]\n",
    "    fitness_scores = torch.tensor(fitnesses)\n",
    "    probabilities = fitness_scores / fitness_scores.sum()\n",
    "\n",
    "    selected_indices = torch.multinomial(probabilities, num_samples=2)\n",
    "    return population[selected_indices]\n",
    "\n",
    "def crossover_cuda(parent1, parent2, connectivity,chiplets):\n",
    "    \"\"\"\n",
    "    Perform crossover between two parents.\n",
    "    input: \n",
    "        parent1: the first parent\n",
    "        parent2: the second parent\n",
    "        chiplets: the chiplet dictionary\n",
    "    output: \n",
    "        the two children\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure parents are on CUDA\n",
    "    parent1 = parent1.to(DEVICE)\n",
    "    parent2 = parent2.to(DEVICE)\n",
    "    \n",
    "    # Initialize result tensors\n",
    "\n",
    "    max_attempts = parent1.size(0)\n",
    "\n",
    "    \n",
    "    result = [[parent1, fitness_cuda(parent1, connectivity, chiplets)], [parent2, fitness_cuda(parent2, connectivity, chiplets)]]\n",
    "    result.sort(key=lambda x: x[1])\n",
    "    \n",
    "    for p in range(1, max_attempts-1): \n",
    "        # point = random.randint(1, len(parent1) - 1)\n",
    "        child1 = torch.cat([parent1[:p].clone(), parent2[p:].clone()], dim=0)\n",
    "        child2 = torch.cat([parent2[:p].clone(), parent1[p:].clone()], dim=0)\n",
    "\n",
    "        \n",
    "        # child2 = parent2[:p] + parent1[p:]\n",
    "\n",
    "        if not check_overlap_within_layout_cuda(child1): \n",
    "            temp = [child1, fitness_cuda(child1, connectivity, chiplets)]\n",
    "            if temp[1] < result[0][1]: \n",
    "                result.insert(0, temp)\n",
    "                result.pop()\n",
    "            elif temp[1] < result[1][1]: \n",
    "                result.insert(1, temp)\n",
    "                result.pop()\n",
    "            # return child1, parent1\n",
    "\n",
    "        if not check_overlap_within_layout_cuda(child2): \n",
    "            temp = [child2, fitness_cuda(child2, connectivity, chiplets)]\n",
    "            if temp[1] < result[0][1]: \n",
    "                result.insert(0, temp)\n",
    "                result.pop()\n",
    "            elif temp[1] < result[1][1]: \n",
    "                result.insert(1, temp)\n",
    "                result.pop()\n",
    "            # return result[0][0], result[1][0]\n",
    "\n",
    "    return result[0][0], result[1][0]\n",
    "\n",
    "def mutate_cuda(individual, chiplets, curr_epoch, max_epoch, max_attempt=10, margin_width=1.0, min_edge_distance=0.2):\n",
    "    \"\"\"\n",
    "    Perform mutation on an individual.\n",
    "    input: \n",
    "        individual: the individual (layout)\n",
    "        chiplets: the chiplet dictionary\n",
    "        curr_epoch: the current epoch\n",
    "        max_epoch: the maximum epoch\n",
    "        max_attempt: the maximum attempt for a mutation\n",
    "        margin_width: the margin width of the board\n",
    "    output: \n",
    "        the mutated individual\n",
    "    \"\"\"\n",
    "\n",
    "    if not torch.is_tensor(individual):\n",
    "        individual = torch.tensor(individual, device=DEVICE)\n",
    "    factor = get_dynamic_coefficient(current_epoch=curr_epoch, total_epochs=max_epoch)\n",
    "\n",
    "    # Precompute bounding box\n",
    "    x_min_val = torch.min(individual[:, 0]).item()\n",
    "    x_max_val = torch.max(individual[:, 0] + individual[:, 2]).item()\n",
    "    y_min_val = torch.min(individual[:, 1]).item()\n",
    "    y_max_val = torch.max(individual[:, 1] + individual[:, 3]).item()\n",
    "    \n",
    "    centerX = (x_min_val + x_max_val) / 2.0\n",
    "    centerY = (y_min_val + y_max_val) / 2.0\n",
    "\n",
    "\n",
    "    for i, chiplet in enumerate(individual):  # make every chiplet has a chance to mutate\n",
    "        \n",
    "        is_mutated = random.random() < 0.8\n",
    "\n",
    "        if is_mutated:\n",
    "            original_chip = chiplet.clone()\n",
    "            attempts = 0\n",
    "            while attempts < max_attempt: \n",
    "                mutate_direction = random.random()\n",
    "\n",
    "                if mutate_direction < 0.85: # 芯片平移突变 (将芯片向水平或垂直方向移动)\n",
    "                    \n",
    "                    # # Decide move direction (0: horizontal, 1: vertical)\n",
    "                    # move_direction = random.randint(0, 1)\n",
    "                    \n",
    "                    \n",
    "                    # upper, lower = factor * 10, factor * (-10)\n",
    "                    # if move_direction == 0:  # Horizontal move\n",
    "                    #     # x_offset = random.uniform(upper, lower)\n",
    "                    #     # Instead of random.uniform(-10*factor, 10*factor):\n",
    "                    #     #   we do something that more likely moves chip toward board center\n",
    "                    #     centerX = (torch.max(individual[:, 0] + individual[:, 2]) + torch.min(individual[:, 0])) / 2\n",
    "                    #     offset_sign = -1 if (chiplet[0] > centerX) else 1  # pull inward if outside center\n",
    "                    #     x_offset = offset_sign * random.uniform(0, 5*factor)  # smaller range\n",
    "\n",
    "                    #     individual[i][0] += x_offset\n",
    "                    # else:  # Vertical move\n",
    "                    #     # y_offset = random.uniform(upper, lower)\n",
    "\n",
    "                    #     centerY = (torch.max(individual[:, 1] + individual[:, 3]) + torch.min(individual[:, 1])) / 2\n",
    "                    #     offset_sign = -1 if (chiplet[0] > centerY) else 1  # pull inward if outside center\n",
    "                    #     y_offset = offset_sign * random.uniform(0, 5*factor)  # smaller range\n",
    "                    #     individual[i][1] += y_offset\n",
    "\n",
    "                    dx = centerX - chiplet[0].item()\n",
    "                    dy = centerY - chiplet[1].item()\n",
    "\n",
    "                    dx = dx - 0.5 * chiplet[2].item() - 0.5 * min_edge_distance if dx > 0 else dx + 0.1\n",
    "                    dy = dy - 0.5 * chiplet[2].item() - 0.5 * min_edge_distance if dy > 0 else dy + 0.1\n",
    "\n",
    "                    step_scale = random.uniform(0, 0.2 *factor) if 0.2 * factor <= 1 else random.uniform(0, 1)\n",
    "                    new_x = chiplet[0] + step_scale * dx\n",
    "                    new_y = chiplet[1] + step_scale * dy\n",
    "\n",
    "                    individual[i][0] = new_x\n",
    "                    individual[i][1] = new_y\n",
    "\n",
    "                    # Check for overlaps\n",
    "                    if check_overlap_within_layout_cuda(individual):\n",
    "                        # 回溯\n",
    "                        individual[i] = original_chip\n",
    "                        attempts += 1\n",
    "                        continue\n",
    "\n",
    "                    # Adjust board coordinates to ensure positive positioning\n",
    "                    # x_min = torch.min(individual[:, 0]).item() # min(chip[0] for chip in individual)\n",
    "                    # y_min = torch.min(individual[:, 1]).item() # min(chip[1] for chip in individual)\n",
    "                    \n",
    "                    # 更新芯片定位点位置\n",
    "                    # for chip in individual: \n",
    "                    #     chip[0] -= (x_min - margin_width)\n",
    "                    #     chip[1] -= (y_min - margin_width)\n",
    "\n",
    "                    # individual[:, 0] -= (x_min - margin_width)\n",
    "                    # individual[:, 1] -= (y_min - margin_width)\n",
    "\n",
    "                    break\n",
    "                    \n",
    "                elif mutate_direction < 0.95: # 芯片换位突变 (交换任意两个芯片的定位点位置) \n",
    "\n",
    "                    swap_i = random.sample(range(len(individual)), 1)[0]\n",
    "\n",
    "                    # individual[i][0], individual[swap_i][0] = individual[swap_i][0], individual[i][0]\n",
    "                    # individual[swap_i][1], individual[i][1] = individual[i][1], individual[swap_i][1]\n",
    "                    temp_x, temp_y = individual[i][0].item(), individual[i][1].item()\n",
    "                    individual[i][0], individual[i][1] = individual[swap_i][0], individual[swap_i][1]\n",
    "                    individual[swap_i][0], individual[swap_i][1] = \\\n",
    "                        torch.tensor(temp_x).to(DEVICE), torch.tensor(temp_y).to(DEVICE)\n",
    "\n",
    "\n",
    "                    if check_overlap_within_layout_cuda(individual): \n",
    "                        # 回溯\n",
    "                        temp_x, temp_y = individual[i][0].item(), individual[i][1].item()\n",
    "                        individual[i][0], individual[i][1] = individual[swap_i][0], individual[swap_i][1]\n",
    "                        individual[swap_i][0], individual[swap_i][1] = \\\n",
    "                            torch.tensor(temp_x).to(DEVICE), torch.tensor(temp_y).to(DEVICE)\n",
    "                        attempts += 1\n",
    "                        continue\n",
    "\n",
    "                    # # Adjust board coordinates to ensure positive positioning\n",
    "                    # x_min = torch.min(individual[:, 0]).item() \n",
    "                    # y_min = torch.min(individual[:, 1]).item() \n",
    "\n",
    "                    # individual[:, 0] -= (x_min - margin_width)\n",
    "                    # individual[:, 1] -= (y_min - margin_width)\n",
    "                    break\n",
    "                    \n",
    "                else: # 芯片旋转突变 (定位点不会改变，但是长宽位置会交换)\n",
    "                    \n",
    "                    # Swap length and width\n",
    "                    temp_val = individual[i][2].item()\n",
    "                    individual[i][2] = individual[i][3]\n",
    "                    individual[i][3] = torch.tensor(temp_val, device=DEVICE)\n",
    "\n",
    "                    # Check for overlaps\n",
    "                    if check_overlap_within_layout_cuda(individual):\n",
    "                        # 回溯\n",
    "                        temp_val = individual[i][2].item()\n",
    "                        individual[i][2] = individual[i][3] #  = individual[i][3], individual[i][2]\n",
    "                        individual[i][3] = torch.tensor(temp_val, device=DEVICE)\n",
    "                        attempts += 1\n",
    "                        continue\n",
    "\n",
    "                    # # 更新芯片定位点位置\n",
    "                    # x_min = torch.min(individual[:, 0]).item() \n",
    "                    # y_min = torch.min(individual[:, 1]).item() \n",
    "\n",
    "                    # individual[:, 0] -= (x_min - margin_width)\n",
    "                    # individual[:, 1] -= (y_min - margin_width)\n",
    "                        \n",
    "                    chiplets[int(individual[i][-1].item())]['len'], chiplets[int(individual[i][-1].item())]['wid'] = \\\n",
    "                        chiplets[int(individual[i][-1].item())]['wid'], chiplets[int(individual[i][-1].item())]['len']\n",
    "\n",
    "    # 更新芯片定位点位置\n",
    "    x_min = torch.min(individual[:, 0]).item() \n",
    "    y_min = torch.min(individual[:, 1]).item() \n",
    "\n",
    "    individual[:, 0] -= (x_min - margin_width)\n",
    "    individual[:, 1] -= (y_min - margin_width)\n",
    "    return individual\n",
    "\n",
    "def elitist_selection_cuda(population, fitnesses, num_elites):\n",
    "    \"\"\"\n",
    "    Perform elitist selection using PyTorch tensors.\n",
    "    input:\n",
    "        population: Tensor of individuals (genes).\n",
    "        fitnesses: Tensor of fitness values corresponding to the population.\n",
    "        num_elites: Number of top individuals to preserve.\n",
    "    output:\n",
    "        Tensor of elite individuals.\n",
    "    \"\"\"\n",
    "    # Ensure tensors are on the same device\n",
    "    if not isinstance(population, torch.Tensor):\n",
    "        population = torch.tensor(population, device=DEVICE)\n",
    "    if not isinstance(fitnesses, torch.Tensor):\n",
    "        fitnesses = torch.tensor(fitnesses, device=DEVICE)\n",
    "    \n",
    "    # Sort population by fitness (ascending, since lower fitness is better)\n",
    "    sorted_indices = torch.argsort(fitnesses)\n",
    "    elites_indices = sorted_indices[:num_elites]\n",
    "    \n",
    "    # Select the top individuals\n",
    "    elites = population[elites_indices]\n",
    "    \n",
    "    return elites\n",
    "\n",
    "def genetic_algorithm_cuda(chiplets, connectivity, generations=200, population_size=50, num_elites=5, convergence_window=20, convergence_threshold=1): \n",
    "    \"\"\"\n",
    "    Implement the genetic algorithm.\n",
    "    input: \n",
    "        chiplets: the chiplet dictionary\n",
    "        connectivity: the connectivity pairs\n",
    "        generations: the maximum generations\n",
    "        population_size: the population size\n",
    "        num_elites: the number of elites\n",
    "        convergence_window: the convergence window\n",
    "        convergence_threshold: the convergence threshold\n",
    "    output: \n",
    "        the best layout\n",
    "    \"\"\"\n",
    "    init_grid_len = (utilities.avg([chip['len'] for chip in chiplets.values()]) + 10) * len(chiplets.keys()) + 10\n",
    "    init_grid_wid = (utilities.avg([chip['wid'] for chip in chiplets.values()]) + 10) * len(chiplets.keys()) + 10\n",
    "    init_start = time.time()\n",
    "    population = torch.stack([\n",
    "        torch.tensor(generate_a_layout_cuda(init_grid_len, init_grid_wid, chiplets), \n",
    "                    device=DEVICE) \n",
    "        for _ in range(population_size)\n",
    "    ])\n",
    "    print(f\"Init population generated, time consumed: {time.time() - init_start:.4f} seconds\")\n",
    "    # Batch process fitness calculations\n",
    "    \n",
    "    \n",
    "    best_fitness_history = []\n",
    "    \n",
    "    for k in tqdm(range(0, generations), desc=\"generation\"): \n",
    "        # print(\"interation starts\")\n",
    "        start_time = time.time()\n",
    "\n",
    "    \n",
    "        fitnesses = torch.stack([\n",
    "            torch.tensor(fitness_cuda(layout, connectivity, chiplets))\n",
    "            for layout in population\n",
    "        ])\n",
    "        # print(f\"after calculating the fitness, time consumed: {time.time() - start_time:.4f} seconds.\")\n",
    "        best_fitness = torch.min(fitnesses)\n",
    "        best_fitness_history.append(best_fitness)\n",
    "\n",
    "        if len(best_fitness_history) >= convergence_window: \n",
    "            recent_window = best_fitness_history[-convergence_window:]\n",
    "            fitness_variation = (max(recent_window) - min(recent_window)) / min(recent_window)\n",
    "\n",
    "            if fitness_variation < convergence_threshold: \n",
    "                print(\"Converged\")\n",
    "                break\n",
    "\n",
    "        elites = elitist_selection_cuda(population, fitnesses, num_elites)\n",
    "\n",
    "        next_generation = elites.clone()\n",
    "\n",
    "        count = 0 # use a count to avoid infinite loop during crossover and mutation\n",
    "        # print(\"Before while\")\n",
    "        while len(next_generation) < population_size: \n",
    "            parent1, parent2 = select_cuda(population, fitnesses)\n",
    "            # parent1, parent2 = parent1, parent2\n",
    "            valid_child = False\n",
    "            # print(\"In crossover\")\n",
    "            start_time1 = time.time()\n",
    "            child1, child2 = crossover_cuda(parent1, parent2, connectivity, chiplets)\n",
    "            # print(f\"End of crossover, time usage: {time.time() - start_time1:.4f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "            if not check_overlap_within_layout_cuda(child1): \n",
    "                # if random.random() < mutation_rate:\n",
    "                # print(\"In mutation\")\n",
    "                start_time1 = time.time()\n",
    "                child1 = mutate_cuda(child1, chiplets, k, generations)\n",
    "                # print(f\"End of mutation, time usage: {time.time() - start_time1:.4f} seconds\")\n",
    "                # child1.extend(get_shape_of_board(child1, chiplets))\n",
    "                next_generation = torch.cat((next_generation, child1.unsqueeze(0)), dim=0)\n",
    "                valid_child = True\n",
    "        \n",
    "            if not check_overlap_within_layout_cuda(child2):\n",
    "                # if random.random() < mutation_rate: \n",
    "                # print(\"In mutation\")\n",
    "                start_time1 = time.time()\n",
    "                child2 = mutate_cuda(child2, chiplets, k, generations)\n",
    "                # print(f\"End of mutation, time usage: {time.time() - start_time1:.4f} seconds\")\n",
    "                # child2.extend(get_shape_of_board(child2, chiplets))\n",
    "                next_generation = torch.cat((next_generation, child2.unsqueeze(0)), dim=0)  \n",
    "                valid_child = True  \n",
    "\n",
    "            if not valid_child: \n",
    "                count += 1\n",
    "                if count > 25: # cannot find a valid child after 100 times, break\n",
    "                    # print(\"Cannot find a valid child after 100 times, early stop generating for this generation.\")\n",
    "                    # print(\"Current population size: \", len(next_generation))\n",
    "                    break\n",
    "        # print(\"interation ends\")\n",
    "        # print(f\"Generation: {k+1} finished! \\nPopulation: {population.size(0)}, time consumed: {time.time() - start_time:.4f} seconds.\")\n",
    "        population = next_generation[:population_size]\n",
    "    \n",
    "    # parallel processing here\n",
    "    fitnesses = torch.stack([\n",
    "        fitness_cuda(layout, connectivity, chiplets) \n",
    "        for layout in population\n",
    "    ])\n",
    "    # best_index = torch.argmin(fitnesses).item() # fitnesses.index(min(fitnesses))\n",
    "    # return population[best_index].tolist()\n",
    "    return population.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = [\n",
    "#     # [9, 9, 15, 140000000],\n",
    "#     [6, 6, 15, 300000000],\n",
    "#     [5, 5, 15, 80000000], \n",
    "#     [4, 4, 15, 120000000]\n",
    "# ]\n",
    "# chiplets = dict()\n",
    "# chip_amount = len(input)\n",
    "# chip_settings = []\n",
    "\n",
    "# for i in range(chip_amount): \n",
    "#     chip_dict = get_chiplet_dict_cuda(chip_length=input[i][0],\n",
    "#                                  chip_width=input[i][1],\n",
    "#                                  Convection_Film_Coefficient=input[i][2],\n",
    "#                                  Internal_Heat_Generation_Magnitude=input[i][3]) # pending fill the arguments\n",
    "#     chiplets[i] = chip_dict\n",
    "# connectivity = [(0, 1), (1, 2), (0,2), ]\n",
    "# generate_a_layout_cuda(100, 100, chiplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the chiplets\n",
    "input = [\n",
    "    [9, 9, 15, 140000000],\n",
    "    [6, 6, 15, 300000000],\n",
    "    [5, 5, 15, 80000000], \n",
    "    # [4, 4, 15, 120000000]\n",
    "]\n",
    "chiplets = dict()\n",
    "chip_amount = len(input)\n",
    "chip_settings = []\n",
    "\n",
    "for i in range(chip_amount): \n",
    "    chip_dict = get_chiplet_dict_cuda(chip_length=input[i][0],\n",
    "                                 chip_width=input[i][1],\n",
    "                                 Convection_Film_Coefficient=input[i][2],\n",
    "                                 Internal_Heat_Generation_Magnitude=input[i][3]) # pending fill the arguments\n",
    "    chiplets[i] = chip_dict\n",
    "connectivity = [\n",
    "    [(0, 1), 1], \n",
    "    [(1, 2), 2], \n",
    "    [(0, 2), 1], \n",
    "    #[(0, 3), 1], \n",
    "    #[(2,3), 1]\n",
    "]\n",
    "design = genetic_algorithm_cuda(chiplets, connectivity, \n",
    "                           population_size=32*chip_amount,\n",
    "                           num_elites=chip_amount*10, \n",
    "                           convergence_window=10*chip_amount, \n",
    "                           generations=500,\n",
    "                           convergence_threshold=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_cuda(design[0], connectivity, chiplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_design = [\n",
    "    [1.0, 1.0, 6.0, 6.0, 0.0],\n",
    "    [7.2, 5.2, 5.0, 5.0, 1.0],\n",
    "    [7.2, 1.0, 4.0, 4.0, 2.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_t = torch.tensor(design, device=DEVICE)\n",
    "design_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.show_chip_design_cuda(design[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[42., 36.,  6.,  6.,  0.],\n",
    "        [20., 22.,  5.,  5.,  1.],\n",
    "        [ 1.,  1.,  4.,  4.,  2.]])\n",
    "\n",
    "a[1][2].item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
